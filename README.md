# Mel-Spectrogram Inpainting for Music Reconstruction ğŸµğŸ§ 

This project explores **music reconstruction via inpainting of Mel-spectrograms** using vision-based deep learning models.  
The goal is to reconstruct missing audio segments by treating the problem as an **image inpainting task**.

---

## ğŸ“Œ Motivation

Audio recordings may contain missing, corrupted, or poorly recorded segments due to noise, transmission errors, or editing issues.  
Rather than directly generating raw audio, we transform the problem into the **Mel-spectrogram domain**, enabling the use of powerful convolutional architectures originally designed for computer vision.

The objective is **not** to perfectly recover the original audio, but to generate a **coherent and perceptually plausible reconstruction**.

---

## ğŸ§  Core Idea

**Pipeline:**

Audio with gap  
â†“ *(torchaudio)*  
Mel-spectrogram with gap  
â†“ *(Mel-Inpainter â€“ CNN / U-Net)*  
Reconstructed Mel-spectrogram  
â†“ *(Griffinâ€“Lim)*  
Reconstructed audio segment


This formulation allows us to leverage **image inpainting techniques** for audio processing.

---

## ğŸ—ï¸ Model Architecture

- Input: Mel-spectrogram context (before & after the gap)
- Model: Convolutional neural network (U-Netâ€“style)
- Output: Missing region of the Mel-spectrogram
- Post-processing: Griffinâ€“Lim algorithm for waveform reconstruction

Baseline models (fully connected) are also implemented for comparison.

---

## ğŸ¼ Dataset

We use the **CAL500 (Computer Audition Lab 500)** dataset:

- 500 songs from Western popular music
- 500 distinct artists
- Average duration â‰ˆ 3 minutes
- Total duration â‰ˆ 27 hours

> âš ï¸ Audio files are **not included** in this repository due to size constraints.

---

## ğŸ”Š Audio Preprocessing

- Sample rate: 22,050 Hz
- Mel bands: 128
- FFT size: 2048
- Hop length: 512  
  â†’ 1 frame â‰ˆ 23 ms

Fixed-size windows are used:
- `context_window`: audio context before & after the gap
- `missing_gap`: duration of the missing segment to reconstruct

---

## Audio Reconstruction

**Griffinâ€“Lim** is used to reconstruct the waveform from the predicted Mel-spectrogram.

> Note: Griffinâ€“Lim is an iterative phase estimation algorithm and does not perfectly recover the original signal.  
> Only the reconstructed missing segment is used; the rest comes from the original audio.

---

## ğŸ“Š Training & Evaluation

- Loss: pixel-wise loss on Mel-spectrograms
- Training curves show stable convergence
- Qualitative evaluation via spectrogram visualization and audio listening

---

## ğŸ“ Repository Structure

```text
mel-spectrogram-inpainting/
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â”‚
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ config.py
â”‚
â”œâ”€â”€ models/                 # Neural network architectures
â”‚   â”œâ”€â”€ fc_baseline.py
â”‚   â”œâ”€â”€ unet.py
â”‚   â””â”€â”€ inpainter.py
â”‚
â”œâ”€â”€ data/                   # Dataset (not versioned)
â”‚   â””â”€â”€ music/              # Audio files (local only)
â”‚
â”œâ”€â”€ datasets/               # PyTorch datasets
â”‚   â”œâ”€â”€ mel_dataset.py
â”‚   â””â”€â”€ data_gen.py
â”‚
â”œâ”€â”€ processors/             # Audio & feature processing
â”‚   â””â”€â”€ audio_processor.py
â”‚
â”œâ”€â”€ training/
â”‚   â””â”€â”€ trainer.py
â”‚
â”œâ”€â”€ scripts/                # Executable scripts
â”‚   â”œâ”€â”€ train.py
â”‚   â””â”€â”€ inference.py
â”‚
â”œâ”€â”€ checkpoints/            # Trained models
â”‚   â””â”€â”€ model_final.pth
â”‚
â””â”€â”€ results/                # Final results
    â”œâ”€â”€ audio/
    â””â”€â”€ figures/
```


---

## Future Work

- Architecture optimization
- More flexible temporal modeling
- Alternative vocoders (e.g., neural vocoders)
- Quantitative perceptual evaluation

---

## ğŸ‘¤ Author

**Adnane Alami**  
**Anas Maillal**  
Project developed for academic purposes in audio & deep learning research.
